{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses Milvus through Docker Compose so you must have Docker installed to run this notebook (Milvus is spun up via `docker compose up -d` as shown in the block below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pymilvus milvus langchain sentence-transformers tiktoken octoai-sdk\n",
    "# docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.octoai_endpoint import OctoAIEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OCTOAI_TOKEN\"] = os.getenv(\"OCTOAI_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OctoAIEndpoint(\n",
    "        model=\"mixtral-8x7b-instruct-fp16\",\n",
    "        max_tokens=200,\n",
    "        presence_penalty=0,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OctoAIEmbeddings\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago.txt',\n",
       " 'Washington, D.C..txt',\n",
       " 'Cambridge, Massachusetts.txt',\n",
       " 'Houston.txt',\n",
       " 'Seattle.txt',\n",
       " 'Toronto.txt',\n",
       " 'San Francisco.txt',\n",
       " 'Boston.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1299, which is longer than the specified 512\n",
      "Created a chunk of size 597, which is longer than the specified 512\n",
      "Created a chunk of size 524, which is longer than the specified 512\n",
      "Created a chunk of size 535, which is longer than the specified 512\n",
      "Created a chunk of size 843, which is longer than the specified 512\n",
      "Created a chunk of size 746, which is longer than the specified 512\n",
      "Created a chunk of size 887, which is longer than the specified 512\n",
      "Created a chunk of size 662, which is longer than the specified 512\n",
      "Created a chunk of size 515, which is longer than the specified 512\n",
      "Created a chunk of size 615, which is longer than the specified 512\n",
      "Created a chunk of size 875, which is longer than the specified 512\n",
      "Created a chunk of size 807, which is longer than the specified 512\n",
      "Created a chunk of size 730, which is longer than the specified 512\n",
      "Created a chunk of size 992, which is longer than the specified 512\n",
      "Created a chunk of size 793, which is longer than the specified 512\n",
      "Created a chunk of size 677, which is longer than the specified 512\n",
      "Created a chunk of size 530, which is longer than the specified 512\n",
      "Created a chunk of size 592, which is longer than the specified 512\n",
      "Created a chunk of size 583, which is longer than the specified 512\n",
      "Created a chunk of size 575, which is longer than the specified 512\n",
      "Created a chunk of size 716, which is longer than the specified 512\n",
      "Created a chunk of size 631, which is longer than the specified 512\n",
      "Created a chunk of size 972, which is longer than the specified 512\n",
      "Created a chunk of size 696, which is longer than the specified 512\n",
      "Created a chunk of size 737, which is longer than the specified 512\n",
      "Created a chunk of size 785, which is longer than the specified 512\n",
      "Created a chunk of size 547, which is longer than the specified 512\n",
      "Created a chunk of size 739, which is longer than the specified 512\n",
      "Created a chunk of size 837, which is longer than the specified 512\n",
      "Created a chunk of size 640, which is longer than the specified 512\n",
      "Created a chunk of size 944, which is longer than the specified 512\n",
      "Created a chunk of size 656, which is longer than the specified 512\n",
      "Created a chunk of size 1213, which is longer than the specified 512\n",
      "Created a chunk of size 1084, which is longer than the specified 512\n",
      "Created a chunk of size 519, which is longer than the specified 512\n",
      "Created a chunk of size 778, which is longer than the specified 512\n",
      "Created a chunk of size 624, which is longer than the specified 512\n",
      "Created a chunk of size 614, which is longer than the specified 512\n",
      "Created a chunk of size 995, which is longer than the specified 512\n",
      "Created a chunk of size 608, which is longer than the specified 512\n",
      "Created a chunk of size 670, which is longer than the specified 512\n",
      "Created a chunk of size 648, which is longer than the specified 512\n",
      "Created a chunk of size 894, which is longer than the specified 512\n",
      "Created a chunk of size 770, which is longer than the specified 512\n",
      "Created a chunk of size 797, which is longer than the specified 512\n",
      "Created a chunk of size 840, which is longer than the specified 512\n",
      "Created a chunk of size 834, which is longer than the specified 512\n",
      "Created a chunk of size 776, which is longer than the specified 512\n",
      "Created a chunk of size 695, which is longer than the specified 512\n",
      "Created a chunk of size 1956, which is longer than the specified 512\n",
      "Created a chunk of size 659, which is longer than the specified 512\n",
      "Created a chunk of size 1033, which is longer than the specified 512\n",
      "Created a chunk of size 626, which is longer than the specified 512\n",
      "Created a chunk of size 970, which is longer than the specified 512\n",
      "Created a chunk of size 714, which is longer than the specified 512\n",
      "Created a chunk of size 672, which is longer than the specified 512\n",
      "Created a chunk of size 600, which is longer than the specified 512\n",
      "Created a chunk of size 567, which is longer than the specified 512\n",
      "Created a chunk of size 684, which is longer than the specified 512\n",
      "Created a chunk of size 1045, which is longer than the specified 512\n",
      "Created a chunk of size 545, which is longer than the specified 512\n",
      "Created a chunk of size 783, which is longer than the specified 512\n",
      "Created a chunk of size 546, which is longer than the specified 512\n",
      "Created a chunk of size 559, which is longer than the specified 512\n",
      "Created a chunk of size 716, which is longer than the specified 512\n",
      "Created a chunk of size 869, which is longer than the specified 512\n",
      "Created a chunk of size 542, which is longer than the specified 512\n",
      "Created a chunk of size 921, which is longer than the specified 512\n",
      "Created a chunk of size 891, which is longer than the specified 512\n",
      "Created a chunk of size 542, which is longer than the specified 512\n",
      "Created a chunk of size 762, which is longer than the specified 512\n",
      "Created a chunk of size 567, which is longer than the specified 512\n",
      "Created a chunk of size 584, which is longer than the specified 512\n",
      "Created a chunk of size 701, which is longer than the specified 512\n",
      "Created a chunk of size 713, which is longer than the specified 512\n",
      "Created a chunk of size 543, which is longer than the specified 512\n",
      "Created a chunk of size 838, which is longer than the specified 512\n",
      "Created a chunk of size 666, which is longer than the specified 512\n",
      "Created a chunk of size 690, which is longer than the specified 512\n",
      "Created a chunk of size 758, which is longer than the specified 512\n",
      "Created a chunk of size 1142, which is longer than the specified 512\n",
      "Created a chunk of size 1014, which is longer than the specified 512\n",
      "Created a chunk of size 531, which is longer than the specified 512\n",
      "Created a chunk of size 1038, which is longer than the specified 512\n",
      "Created a chunk of size 585, which is longer than the specified 512\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    with open(f\"./data/{file}\") as f:\n",
    "        file_text = f.read()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512, chunk_overlap=64, \n",
    "    )\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text, \n",
    "                metadata={\"doc_title\": file.split(\".\")[0], \"chunk_num\": i}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Chicago (  shih-KAH-goh, locally also  shih-KAW-goh; Miami-Illinois: Shikaakwa; Ojibwe: Zhigaagong) is the most populous city in the U.S. state of Illinois and the third-most populous in the United States after New York City and Los Angeles. With a population of 2,746,388 in the 2020 census, it is also the most populous city in the Midwest. As the seat of Cook County, the second-most populous county in the U.S., Chicago is the center of the Chicago metropolitan area.\\nLocated on the shore of Lake Michigan, Chicago was incorporated as a city in 1837 near a portage between the Great Lakes and the Mississippi River watershed. It grew rapidly in the mid-19th century. In 1871, the Great Chicago Fire destroyed several square miles and left more than 100,000 homeless, but Chicago's population continued to grow. Chicago made noted contributions to urban planning and architecture, such as the Chicago School, the development of the City Beautiful Movement, and the steel-framed skyscraper.Chicago is an international hub for finance, culture, commerce, industry, education, technology, telecommunications, and transportation. It has the largest and most diverse derivatives market in the world, generating 20% of all volume in commodities and financial futures alone. O'Hare International Airport is routinely ranked among the world's top six busiest airports by passenger traffic, and the region is also the nation's railroad hub. The Chicago area has one of the highest gross domestic products (GDP) in the world, generating $689 billion in 2018. Chicago's economy is diverse, with no single industry employing more than 14% of the workforce.Chicago is a major tourist destination. Chicago's culture has contributed much to the visual arts, literature, film, theater, comedy (especially improvisational comedy), food, dance, and music (particularly jazz, blues, soul, hip-hop, gospel, and electronic dance music, including house music). Chicago is home to the Chicago Symphony Orchestra and the Lyric Opera of Chicago. The Chicago area also hosts the University of Chicago, Northwestern University, and the University of Illinois Chicago, among other institutions of learning. Chicago has professional sports teams in each of the major professional leagues, including two Major League Baseball teams.\\n\\n\\n== Etymology and nicknames ==\", metadata={'doc_title': 'Chicago', 'chunk_num': 0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Milvus.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings,\n",
    "    connection_args={\"host\": \"localhost\", \"port\": 19530},\n",
    "    collection_name=\"cities\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Seattle is a significant city with a 2022 population of 749,256 people, making it the most populous city in both the state of Washington and the Pacific Northwest region of North America. The Seattle metropolitan area's population is 4.02 million, making it the 15th-largest in the United States.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"How big is Seattle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this a bit more fun and showcase the multilingual capabilities of Mixtal which really outshine other open source models\n",
    "\n",
    "# Our Vector DB is populated with entries from english text - even the embedding model we're using here, GTE-Large\n",
    "# works best on english text. However Mixtral has good mutlilingual capabilities in French, German, Spanish and Italian.\n",
    "# So what we'll do is ask the assistant to only answer in french in the system and user prompt. RAG here is performed based on \n",
    "# english text, but upon producing the user response, the Mixtral LLM will generate tokens in a different language here (french)\n",
    "french_llm = OctoAIEndpoint(\n",
    "        model=\"mixtral-8x7b-instruct-fp16\",\n",
    "        max_tokens=200,\n",
    "        presence_penalty=0,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "french_template = \"\"\"Answer the question in French based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "french_prompt = PromptTemplate.from_template(french_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | french_prompt\n",
    "    | french_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_1 = french_chain.invoke(\"How big is the city of Seattle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(fr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_2 = french_chain.invoke(\"When should I visit Seattle?\")\n",
    "pprint(fr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_3 = french_chain.invoke(\"What are some things to do Seattle?\")\n",
    "pprint(fr_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "template = \"\"\"Translate the entry into english {entry} \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(llm_chain.invoke(fr_1)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
